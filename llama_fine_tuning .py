# -*- coding: utf-8 -*-
"""llama_fine_tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N9S-WYJTPMlCN4cS9H9GIE5GIHhHfwRB
"""

# !pip install bitsandbytes

!huggingface-cli login

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, confusion_matrix
import torch
import os
from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    TrainingArguments,
    Trainer
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from datasets import Dataset
import warnings
warnings.filterwarnings('ignore')

# Set environment variables for better stability
os.environ["TOKENIZERS_PARALLELISM"] = "false"
os.environ["CUDA_LAUNCH_BLOCKING"] = "1"  # For better error messages

# ============================================================================
# 1. DATA PREPARATION AND SPLITTING
# ============================================================================

def load_and_split_data(csv_path, test_size=0.2, val_size=0.1):
    """Load CSV and split into train/val/test (70:10:20)"""
    df = pd.read_csv(csv_path)

    # Check class distribution
    print("\n" + "="*60)
    print("CLASS DISTRIBUTION IN FULL DATASET")
    print("="*60)
    print(df['category'].value_counts().sort_index())
    print(f"\nTotal samples: {len(df)}")

    # First split: separate test set (20%)
    train_val, test = train_test_split(
        df,
        test_size=test_size,
        random_state=42,
        stratify=df['category']
    )

    # Second split: separate validation from training (10% of total = 12.5% of train_val)
    val_ratio = val_size / (1 - test_size)
    train, val = train_test_split(
        train_val,
        test_size=val_ratio,
        random_state=42,
        stratify=train_val['category']
    )

    print(f"\nData split sizes:")
    print(f"Train: {len(train)} ({len(train)/len(df)*100:.1f}%)")
    print(f"Val: {len(val)} ({len(val)/len(df)*100:.1f}%)")
    print(f"Test: {len(test)} ({len(test)/len(df)*100:.1f}%)")

    print("\n" + "="*60)
    print("TRAINING SET CLASS DISTRIBUTION")
    print("="*60)
    print(train['category'].value_counts().sort_index())

    return train, val, test

def prepare_instruction_dataset(df, tokenizer, max_length=256):
    """Convert dataframe to instruction-tuning format"""

    # Get all unique categories once
    all_categories = sorted(df['category'].unique().tolist())
    categories_str = ", ".join(all_categories)

    def format_instruction(row):
        instruction = f"""Classify the following transaction into the correct category.

Available categories: {categories_str}

Transaction: {row['transaction_text']}

Category:"""

        full_text = f"{instruction} {row['category']}"
        return full_text, instruction, row['category']

    # Create formatted texts with separate instruction and response
    formatted_data = df.apply(format_instruction, axis=1, result_type='expand')
    formatted_data.columns = ['full_text', 'instruction', 'response']

    texts = formatted_data['full_text'].tolist()
    instructions = formatted_data['instruction'].tolist()

    # Tokenize full text
    encodings = tokenizer(
        texts,
        truncation=True,
        padding='max_length',
        max_length=max_length,
        return_tensors='pt'
    )

    # Tokenize instructions only (to mask them in loss calculation)
    instruction_encodings = tokenizer(
        instructions,
        truncation=True,
        padding='max_length',
        max_length=max_length,
        return_tensors='pt'
    )

    # Create labels: mask instruction tokens, only compute loss on response
    labels = encodings['input_ids'].clone()
    instruction_lengths = (instruction_encodings['input_ids'] != tokenizer.pad_token_id).sum(dim=1)

    # Mask instruction part with -100 (ignored in loss calculation)
    for idx, inst_len in enumerate(instruction_lengths):
        labels[idx, :inst_len] = -100

    # Also mask padding tokens
    labels[labels == tokenizer.pad_token_id] = -100

    # Create dataset
    dataset = Dataset.from_dict({
        'input_ids': encodings['input_ids'],
        'attention_mask': encodings['attention_mask'],
        'labels': labels
    })

    return dataset

# ============================================================================
# 2. MODEL SETUP WITH QLORA
# ============================================================================

def setup_model_and_tokenizer(model_name="meta-llama/Llama-3.1-8B-Instruct"):
    """Setup model with QLoRA configuration"""

    # Clear CUDA cache
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    # QLoRA configuration - 4-bit quantization
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16  # Changed from bfloat16 for compatibility
    )

    # Load tokenizer
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True,
        use_fast=True
    )

    # Set padding token
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
        tokenizer.pad_token_id = tokenizer.eos_token_id
    tokenizer.padding_side = "right"

    # Load model with quantization
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        quantization_config=bnb_config,
        device_map="auto",
        trust_remote_code=True,
        torch_dtype=torch.float16,
        low_cpu_mem_usage=True
    )

    # Disable cache for training
    model.config.use_cache = False
    model.config.pretraining_tp = 1

    # Prepare model for k-bit training
    model = prepare_model_for_kbit_training(model)

    # Enable gradient checkpointing
    model.gradient_checkpointing_enable()

    # LoRA configuration
    lora_config = LoraConfig(
        r=16,  # Rank
        lora_alpha=32,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )

    # Add LoRA adapters
    model = get_peft_model(model, lora_config)

    print(f"Trainable parameters:")
    model.print_trainable_parameters()

    return model, tokenizer

# ============================================================================
# 3. TRAINING
# ============================================================================

def train_model(model, tokenizer, train_dataset, val_dataset, output_dir="./results"):
    """Fine-tune model using QLoRA"""

    training_args = TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=5,  # Increased from 3 to 5
        per_device_train_batch_size=2,
        per_device_eval_batch_size=2,
        gradient_accumulation_steps=8,
        learning_rate=3e-4,  # Increased from 2e-4 for faster learning
        fp16=True,
        logging_steps=5,  # More frequent logging
        eval_strategy="steps",
        eval_steps=25,  # More frequent evaluation
        save_strategy="steps",
        save_steps=50,
        save_total_limit=3,  # Keep more checkpoints
        load_best_model_at_end=True,
        warmup_steps=50,  # Reduced warmup
        weight_decay=0.01,
        report_to="none",
        optim="paged_adamw_8bit",
        gradient_checkpointing=True,
        max_grad_norm=0.3,
        lr_scheduler_type="cosine",
        metric_for_best_model="eval_loss",
        greater_is_better=False
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=val_dataset,
    )

    print("\nStarting training...")

    # Clear cache before training
    if torch.cuda.is_available():
        torch.cuda.empty_cache()

    trainer.train()

    return trainer

# ============================================================================
# 4. EVALUATION AND METRICS
# ============================================================================

def predict_category(text, model, tokenizer, categories):
    """Predict category for a single transaction"""

    categories_str = ", ".join(sorted(categories))

    prompt = f"""Classify the following transaction into the correct category.

Available categories: {categories_str}

Transaction: {text}

Category:"""

    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=30,  # Increased for longer category names
            temperature=0.01,  # Very low for deterministic output
            do_sample=False,
            pad_token_id=tokenizer.eos_token_id,
            repetition_penalty=1.1  # Reduce repetition
        )

    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    predicted = response.split("Category:")[-1].strip().split("\n")[0].strip()

    # Remove any extra text after the category
    predicted = predicted.split(',')[0].split('.')[0].strip()

    # Exact match first
    if predicted in categories:
        return predicted

    # Case-insensitive match
    predicted_lower = predicted.lower()
    for cat in categories:
        if cat.lower() == predicted_lower:
            return cat

    # Partial match (predicted contains category or vice versa)
    for cat in categories:
        cat_lower = cat.lower()
        if cat_lower in predicted_lower or predicted_lower in cat_lower:
            return cat

    # Fuzzy matching for common variations
    category_mappings = {
        'bills': 'Bills & Utilities',
        'utilities': 'Bills & Utilities',
        'cash': 'Cash Withdrawal',
        'withdrawal': 'Cash Withdrawal',
        'coffee': 'Coffee & Dining',
        'dining': 'Coffee & Dining',
        'restaurant': 'Coffee & Dining',
        'donation': 'Donations',
        'education': 'Education',
        'school': 'Education',
        'electronics': 'Electronics',
        'entertainment': 'Entertainment',
        'movie': 'Entertainment',
        'delivery': 'Food Delivery',
        'fuel': 'Fuel',
        'gas': 'Fuel',
        'petrol': 'Fuel',
        'grocery': 'Groceries',
        'groceries': 'Groceries',
        'supermarket': 'Groceries',
        'health': 'Healthcare',
        'healthcare': 'Healthcare',
        'medical': 'Healthcare',
        'insurance': 'Insurance',
        'shop': 'Shopping',
        'shopping': 'Shopping',
        'transport': 'Transport',
        'taxi': 'Transport',
        'uber': 'Transport',
        'travel': 'Travel',
        'hotel': 'Travel',
        'flight': 'Travel'
    }

    for keyword, category in category_mappings.items():
        if keyword in predicted_lower:
            return category

    # If still no match, return most common category as fallback
    print(f"Warning: Could not match '{predicted}' to any category. Using fallback.")
    return categories[0]

def evaluate_model(model, tokenizer, test_df):
    """Generate classification report"""

    print("\n" + "="*60)
    print("EVALUATING MODEL ON TEST SET")
    print("="*60)

    categories = sorted(test_df['category'].unique().tolist())
    predictions = []

    # Reset index to avoid confusion
    test_df_reset = test_df.reset_index(drop=True)
    total_samples = len(test_df_reset)

    print(f"\nTotal test samples: {total_samples}")
    print(f"Number of categories: {len(categories)}")
    print(f"Categories: {', '.join(categories)}\n")

    for idx, row in test_df_reset.iterrows():
        pred = predict_category(row['transaction_text'], model, tokenizer, categories)
        predictions.append(pred)

        if (idx + 1) % 10 == 0:
            print(f"Processed {idx + 1}/{total_samples} samples...")

    print(f"Processed {total_samples}/{total_samples} samples... Done!\n")

    # Classification report
    print("=" * 60)
    print("CLASSIFICATION REPORT")
    print("=" * 60)
    print(classification_report(
        test_df['category'],
        predictions,
        labels=categories,
        zero_division=0,
        digits=4
    ))

    # Overall accuracy
    correct = sum([1 for true, pred in zip(test_df['category'], predictions) if true == pred])
    accuracy = correct / len(predictions) * 100
    print(f"\nOverall Accuracy: {accuracy:.2f}% ({correct}/{len(predictions)})")

    # Confusion matrix
    print("\n" + "="*60)
    print("CONFUSION MATRIX")
    print("="*60)
    cm = confusion_matrix(test_df['category'], predictions, labels=categories)
    cm_df = pd.DataFrame(cm, index=categories, columns=categories)
    print(cm_df)

    # Per-category accuracy
    print("\n" + "="*60)
    print("PER-CATEGORY ACCURACY")
    print("="*60)
    for i, cat in enumerate(categories):
        if cm[i].sum() > 0:
            cat_acc = cm[i][i] / cm[i].sum() * 100
            print(f"{cat:30s}: {cat_acc:6.2f}% ({cm[i][i]}/{cm[i].sum()})")

    return predictions

# ============================================================================
# MAIN EXECUTION
# ============================================================================

if __name__ == "__main__":

    # Configuration
    CSV_PATH = "synthetic_transactions.csv"
    MODEL_NAME = "meta-llama/Llama-3.1-8B-Instruct"
    OUTPUT_DIR = "./llama_qlora_results"

    print("="*60)
    print("TRANSACTION CLASSIFICATION WITH QLORA FINE-TUNING")
    print("="*60)

    # Step 1: Load and split data
    print("\n[1/4] Loading and splitting data...")
    train_df, val_df, test_df = load_and_split_data(CSV_PATH)

    # Step 2: Setup model and tokenizer
    print("\n[2/4] Setting up model with QLoRA...")
    model, tokenizer = setup_model_and_tokenizer(MODEL_NAME)

    # Prepare datasets
    print("\n[2/4] Preparing instruction datasets...")
    train_dataset = prepare_instruction_dataset(train_df, tokenizer)
    val_dataset = prepare_instruction_dataset(val_df, tokenizer)

    # Step 3: Train model
    print("\n[3/4] Fine-tuning model...")
    trainer = train_model(model, tokenizer, train_dataset, val_dataset, OUTPUT_DIR)

    # Step 4: Evaluate
    print("\n[4/4] Evaluating on test set...")
    predictions = evaluate_model(model, tokenizer, test_df)

    # Save results
    test_df['predicted_category'] = predictions
    test_df.to_csv(f"{OUTPUT_DIR}/test_predictions.csv", index=False)
    print(f"\nPredictions saved to {OUTPUT_DIR}/test_predictions.csv")

    print("\n" + "="*60)
    print("TRAINING AND EVALUATION COMPLETE!")
    print("="*60)

